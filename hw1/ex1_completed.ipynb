{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear Image Classifier\n",
    "\n",
    "In this exercise you will implement a linear image classifier while getting familiar with `numpy` and the benefits of vectorized operations in Python.\n",
    "\n",
    "## Environment\n",
    "\n",
    "- Platform: Google Colab, CPU runtime (no GPU).\n",
    "- Set a global random seed for reproducibility (e.g., np.random.seed(42))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# TODO: Replace 'your_project_folder' with the actual path to your project folder in Google Drive\n",
    "project_folder = '/content/drive/MyDrive/Deep-Learning-HW1/hw1'\n",
    "sys.path.append(project_folder)\n",
    "print(f\"Added {project_folder} to system path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import itertools\n",
    "import time\n",
    "import linear_models\n",
    "import importlib\n",
    "importlib.reload(linear_models)\n",
    "from typing import Dict, Tuple, Iterable, Optional, Any\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# specify the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 3.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, download_dir):\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "        print(\"Downloading, This might take several minutes.\")\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url, filename=file_path)\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "        \n",
    "        if file_path.endswith(\".zip\"):\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "        print(\"Done. Dataset is ready!\")\n",
    "    else:\n",
    "        print(\"Dataset already downloaded and unpacked.\")\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b,))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "PATH = 'datasets/cifar10/'\n",
    "download_and_extract(URL, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
    "X_train, y_train, X_test, y_test = load(CIFAR10_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❀ Q1: Exploratory Data Analysis (EDA) ❀\n",
    "\n",
    "**(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Print the shapes of the training and test sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print()\n",
    "\n",
    "# (2) Display the number of classes and their names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(class_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print()\n",
    "\n",
    "# (3) Show the class distribution in the training set\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution in training set:\")\n",
    "for cls_idx, count in zip(unique, counts):\n",
    "    print(f\"  Class {cls_idx} ({class_names[cls_idx]}): {count} samples\")\n",
    "print(f\"Total training samples: {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only the target classes\n",
    "TARGET_CLASSES = [2, 3, 4]\n",
    "classes = ['bird', 'cat', 'deer']\n",
    "\n",
    "train_mask = np.isin(y_train, TARGET_CLASSES)\n",
    "test_mask = np.isin(y_test, TARGET_CLASSES)\n",
    "\n",
    "X_train = X_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "X_test = X_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "\n",
    "# Relabel to {0,1,2}\n",
    "label_map = {orig: i for i, orig in enumerate(TARGET_CLASSES)}\n",
    "y_train = np.vectorize(label_map.get)(y_train)\n",
    "y_test = np.vectorize(label_map.get)(y_test)\n",
    "\n",
    "# Define sizes\n",
    "num_training = 10000\n",
    "num_validation = 1000\n",
    "num_testing = 1000\n",
    "\n",
    "# Create subsets\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = range(num_validation)\n",
    "X_val = X_test[mask]\n",
    "y_val = y_test[mask]\n",
    "\n",
    "mask = range(num_validation, num_validation + num_testing)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_val = X_val.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "print(\"Shapes ->\",\n",
    "      \"X_train\", X_train.shape, \"y_train\", y_train.shape,\n",
    "      \"X_val\", X_val.shape, \"y_val\", y_val.shape,\n",
    "      \"X_test\", X_test.shape, \"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, n=1000):\n",
    "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
    "    images = X[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    return images, labels\n",
    "\n",
    "def make_random_grid(x, y, n=4, convert_to_image=True, random_flag=True):\n",
    "    if random_flag:\n",
    "        rand_items = np.random.randint(0, x.shape[0], size=n)\n",
    "    else:\n",
    "        rand_items = np.arange(0, x.shape[0])\n",
    "    images = x[rand_items]\n",
    "    labels = y[rand_items]\n",
    "    if convert_to_image:\n",
    "        grid = np.hstack(np.array([np.asarray((vec_2_img(i) + mean_image), dtype=np.int64) for i in images]))\n",
    "    else:\n",
    "        grid = np.hstack(np.array([np.asarray(i, dtype=np.int64) for i in images]))\n",
    "    print('\\t'.join('%9s' % classes[labels[j]] for j in range(n)))\n",
    "    return grid\n",
    "\n",
    "def vec_2_img(x):\n",
    "    x = np.reshape(x[:-1], (32, 32, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = get_batch(X_test, y_test, 100)\n",
    "plt.imshow(make_random_grid(X_batch, y_batch, n=4, convert_to_image=False))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) subtract the TRAIN mean image (feature-wise)\n",
    "mean_image = np.mean(X_train, axis=0, keepdims=True)\n",
    "X_train = X_train - mean_image\n",
    "X_val = X_val - mean_image\n",
    "X_test = X_test - mean_image\n",
    "\n",
    "# 2) flatten HxWxC -> D\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# 3) add a bias term (last feature = 1)\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "print(f\"Shape of Training Set: {X_train.shape}\")\n",
    "print(f\"Shape of Validation Set: {X_val.shape}\")\n",
    "print(f\"Shape of Test Set: {X_test.shape}\")\n",
    "\n",
    "num_classes = int(np.max(y_train)) + 1\n",
    "assert set(np.unique(y_train)) <= set(range(num_classes)), \"y must be in 0..C-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❀ Q2: Understanding the Image Preprocessing ❀\n",
    "\n",
    "**(5 points)**\n",
    "\n",
    "**1. Mean Subtraction**\n",
    "\n",
    "We subtract the mean to center our data around zero. This helps with training because it makes the gradient descent work better - when you have data centered around zero, the updates are more balanced and convergence happens faster. It also removes things like average brightness differences between images so the model can focus on actual features instead of lighting conditions.\n",
    "\n",
    "**2. Flattening**\n",
    "\n",
    "Linear models need 1D vectors as input, not 2D/3D arrays. So we flatten each 32x32x3 image into a single vector of 3072 numbers. This way we can do the matrix multiplication $XW$ where each pixel becomes its own feature. Basically the model treats every pixel position as an independent input.\n",
    "\n",
    "**3. Bias Trick**\n",
    "\n",
    "Instead of keeping track of weights W and bias b separately, we just add a column of 1s to our data. This lets us combine them into one matrix multiplication - the last row of W becomes the bias. Makes the code simpler and cleaner since everything is just one big matrix operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = linear_models.LinearPerceptron(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show = 8\n",
    "idxs = np.random.choice(len(X_test), size=num_show, replace=False)\n",
    "fig, axes = plt.subplots(1, num_show, figsize=(16, 3))\n",
    "\n",
    "mean_flat = mean_image.reshape(-1)\n",
    "if mean_flat.size > 3072:\n",
    "    mean_flat = mean_flat[:3072]\n",
    "mean_img_3d = mean_flat.reshape(32, 32, 3)\n",
    "\n",
    "for ax, i in zip(axes, idxs):\n",
    "    vec = X_test[i].reshape(-1)\n",
    "    if vec.size > 3072:\n",
    "        vec = vec[:3072]\n",
    "    img = vec[:3072].reshape(32, 32, 3) + mean_img_3d\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    true_lbl = classes[y_test[i]]\n",
    "    pred_lbl = classes[y_pred[i]]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Pred: {pred_lbl}\\nTrue: {true_lbl}\",\n",
    "                 color=\"green\" if y_pred[i] == y_test[i] else \"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model accuracy (before training):\", classifier.calc_accuracy(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❀ Q3 ❀\n",
    "\n",
    "**(5 points)**\n",
    "\n",
    "The accuracy is low because we haven't trained the model yet - the weights are just random numbers right now. With 3 classes, random guessing would give about 33% accuracy, which is basically what we're seeing. The model has no idea what features to look for to distinguish birds from cats from deer. Once we train it with gradient descent, it'll learn which pixel patterns correspond to each class and the accuracy will go up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 3) * 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_naive, grad_naive = linear_models.perceptron_loss_naive(W, X_val, y_val)\n",
    "print(\"Loss:\", loss_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_vectorized, _ = linear_models.softmax_cross_entropy(W, X_val, y_val)\n",
    "print('loss: %f' % (loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "perceptron = linear_models.LinearPerceptron(X_train, y_train)\n",
    "loss_history = perceptron.train(X_train, y_train, learning_rate=1e-7,\n",
    "                                num_iters=1500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history, color='c')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy:\", perceptron.calc_accuracy(X_train, y_train))\n",
    "print(\"Testing accuracy:\", perceptron.calc_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization for Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-3, 1e-2, 1e-1]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "results, best_perc, best_val = linear_models.tune_perceptron(\n",
    "    linear_models.LinearPerceptron,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    learning_rates, batch_sizes,\n",
    "    num_iters=500,\n",
    "    model_kwargs=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "for (lr, bs), (tr, va) in sorted(results.items()):\n",
    "    print(f\"lr={lr: .1e} bs={bs:4d}  train_acc={tr:.4f}  val_acc={va:.4f}\")\n",
    "\n",
    "print(f\"best validation accuracy: {best_val:.4f}\")\n",
    "test_acc = best_perc.calc_accuracy(X_test, y_test)\n",
    "print(f\"final test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_models.LogisticRegression(X_train, y_train)\n",
    "y_pred = logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_show = 8\n",
    "idxs = np.random.choice(len(X_test), size=num_show, replace=False)\n",
    "fig, axes = plt.subplots(1, num_show, figsize=(16, 3))\n",
    "\n",
    "mean_flat = np.asarray(mean_image).reshape(-1)\n",
    "mean_img_3d = mean_flat[:3072].reshape(32, 32, 3)\n",
    "\n",
    "for ax, i in zip(axes, idxs):\n",
    "    vec = np.asarray(X_test[i]).reshape(-1)[:3072]\n",
    "    img = vec.reshape(32, 32, 3) + mean_img_3d\n",
    "    vmin, vmax = img.min(), img.max()\n",
    "    if vmax > vmin:\n",
    "        img_disp = (img - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        img_disp = np.zeros_like(img)\n",
    "    \n",
    "    true_lbl = classes[y_test[i]]\n",
    "    pred_lbl = classes[y_pred[i]]\n",
    "    ax.imshow(img_disp)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\n",
    "        f\"Pred: {pred_lbl}\\nTrue: {true_lbl}\",\n",
    "        color=(\"green\" if y_pred[i] == y_test[i] else \"red\")\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model accuracy (before training):\", logistic.calc_accuracy(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 3) * 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_val, grad_val = linear_models.softmax_cross_entropy(W, X_val, y_val)\n",
    "print(f\"loss: {loss_val:.6f}\")\n",
    "print(\"grad shape:\", grad_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        fxph = f(x)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x)\n",
    "        x[ix] = oldval\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "        print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grad = linear_models.softmax_cross_entropy(W, X_val, y_val)\n",
    "f = lambda w: linear_models.softmax_cross_entropy(w, X_val, y_val)[0]\n",
    "grad_numerical = grad_check(f, W, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logistic = linear_models.LogisticRegression(X_train, y_train)\n",
    "loss_history = logistic.train(X_train, y_train,\n",
    "                         learning_rate=1e-7,\n",
    "                         num_iters=1500,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history, color='pink')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy:\", logistic.calc_accuracy(X_train, y_train))\n",
    "print(\"Testing accuracy:\", logistic.calc_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-3, 1e-2, 1e-1]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "\n",
    "results, best_perc, best_val = linear_models.tune_perceptron(\n",
    "    linear_models.LogisticRegression,\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    learning_rates, batch_sizes,\n",
    "    num_iters=500,\n",
    "    model_kwargs=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "for (lr, bs), (tr, va) in sorted(results.items()):\n",
    "    print(f\"lr={lr: .1e} bs={bs:4d}  train_acc={tr:.4f}  val_acc={va:.4f}\")\n",
    "\n",
    "print(f\"best validation accuracy: {best_val:.4f}\")\n",
    "test_acc = best_perc.calc_accuracy(X_test, y_test)\n",
    "print(f\"final test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
