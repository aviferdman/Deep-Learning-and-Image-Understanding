{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpGv7oW4PwdM"
      },
      "source": [
        "# Exercise 2: Neural Networks\n",
        "\n",
        "\n",
        "## Environment\n",
        "- Platform: Google Colab\n",
        "- Set a global random seed for reproducibility (e.g., np.random.seed(42)).\n",
        "\n",
        "\n",
        "## Read the following instructions carefully:\n",
        "\n",
        "1. **Follow the Guide:** This Jupyter Notebook contains all the step-by-step instructions needed for this exercise. Fill in the missing parts wherever marked with TODO.\n",
        "2. **Vectorization:** Write efficient vectorized code. Avoid for-loops unless explicitly allowed. Inefficient code that acts as a bottleneck will be penalized.\n",
        "3. **Correctness & Testing:** You are responsible for the correctness of your code. Add as many test cells as you see fit to verify your logic. (Note: your tests will not be graded).\n",
        "4. **Immutable Functions:** Do not change the signatures or names of the functions we provided.\n",
        "5. **Allowed Libraries:** You are allowed to use functions and methods from the Python Standard Library and numpy only. Any other imports (such as torch, sklearn, etc.) are strictly forbidden.\n",
        "6. **Runtime Efficiency:** Your code must run within a reasonable time. We will not grade notebooks that take an excessive amount of time to execute due to inefficient implementation.\n",
        "7. **Qualitative Answers:** Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
        "\n",
        "\n",
        "## Submission guidelines:\n",
        "- What to submit: A single notebook file named ID_ex2.ipynb (e.g., 123456789_ex2.ipynb).\n",
        "- Your submitted notebook should **run without problems**.\n",
        "- Please submit your **executed** (fully run) notebook, including all outputs (plots, printed results, etc.). Make sure that all cells have been run in order from top to bottom before submission.\n",
        "\n",
        "## Academic integrity\n",
        "\n",
        "You may discuss ideas, but all submitted code must be your own. Cite any external snippets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:18.152048Z",
          "start_time": "2022-11-28T06:46:17.469849Z"
        },
        "id": "pA0hjtyJPwdO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import zipfile\n",
        "from random import randrange\n",
        "from functools import partial\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "# specify the way plots behave in jupyter notebook\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12.0, 12.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcYX2rU2PwdP"
      },
      "source": [
        "# Data preprocessing\n",
        "In this section, we will use the same code as in Homework 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa-bjEWl9fW6"
      },
      "source": [
        "## Data download and processing Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:18.167551Z",
          "start_time": "2022-11-28T06:46:18.153952Z"
        },
        "id": "9_uAlYcQ9dF7"
      },
      "outputs": [],
      "source": [
        "def download_and_extract(url, download_dir):\n",
        "    \"\"\"\n",
        "    Download and extract the CIFAR-10 dataset if it doesn't already exist.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        Internet URL for the tar-file to download.\n",
        "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    download_dir : str\n",
        "        Directory where the downloaded file will be saved and extracted.\n",
        "        Example: \"data/CIFAR-10/\"\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine filename and full path where the file will be saved\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if dataset is already downloaded (and extracted)\n",
        "    if not os.path.exists(file_path):\n",
        "       # Create the directory if it doesn’t exist\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        # Download the dataset\n",
        "        print(\"Downloading, This might take several minutes.\")\n",
        "        last_update_time = time.time()\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url, filename=file_path)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "\n",
        "        # Extract the dataset\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            # Unpack the zip-file.\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            # Unpack the tar-ball.\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "        print(\"Done. Dataset is ready!\")\n",
        "    else:\n",
        "        print(\"Dataset already downloaded and unpacked.\")\n",
        "        print(\"If something seems wrong, delete the folder and re-run.\")\n",
        "\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "    ''' Load a single batch of the CIFAR-10 dataset.'''\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding = 'latin1')\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "\n",
        "        # Reshape and transpose: original shape (10000, 3072)\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "def load(ROOT):\n",
        "    ''' Load all training and test batches of CIFAR-10.'''\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1, 6):\n",
        "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "        X, Y = load_CIFAR_batch(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    del X, Y\n",
        "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "    return Xtr, Ytr, Xte, Yte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MeVWzF19mVA"
      },
      "source": [
        "## Data Download\n",
        "\n",
        "In this section, we will download and extract the **CIFAR-10** dataset if it is not already available locally.\n",
        "\n",
        "- The dataset will be saved under: `datasets/cifar10/`\n",
        "- If it’s already there, the script will **skip downloading**.\n",
        "- This process may take a few minutes the first time you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:18.183294Z",
          "start_time": "2022-11-28T06:46:18.170425Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bHbknwvPwdQ",
        "outputId": "9de3bd25-d503-48ab-d795-26ba5877d41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading, This might take several minutes.\n",
            "\n",
            "Download finished. Extracting files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-787463260.py:43: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Dataset is ready!\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Data Download (Run once)\n",
        "# ---------------------------------------------------------------------\n",
        "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "PATH = 'datasets/cifar10/'\n",
        "download_and_extract(URL, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqwgneo5FchN"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Load the full CIFAR-10 dataset from local path\n",
        "# ---------------------------------------------------------------------\n",
        "CIFAR10_PATH = os.path.join(PATH, 'cifar-10-batches-py')\n",
        "X_train, y_train, X_test, y_test = load(CIFAR10_PATH)  # load the entire data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beFt0AfB9w9f"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_YTIdyB5qMO"
      },
      "source": [
        "**Notice that we are leaving behind the bias trick in this exercise.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:20.967912Z",
          "start_time": "2022-11-28T06:46:18.185329Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyLpzTyzPwdQ",
        "outputId": "281c7468-d08d-4110-e51e-1143ab6d61b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes -> X_train (10000, 32, 32, 3) y_train (10000,) X_val (1000, 32, 32, 3) y_val (1000,) X_test (1000, 32, 32, 3) y_test (1000,)\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Filter the dataset to include only the target classes\n",
        "# ------------------------------------------/---------------------------\n",
        "TARGET_CLASSES = [2, 3, 4]\n",
        "classes = ['bird', 'cat', 'deer']\n",
        "train_mask = np.isin(y_train, TARGET_CLASSES)\n",
        "test_mask  = np.isin(y_test,  TARGET_CLASSES)\n",
        "\n",
        "X_train = X_train[train_mask]\n",
        "y_train = y_train[train_mask]\n",
        "X_test  = X_test[test_mask]\n",
        "y_test  = y_test[test_mask]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Relabel to {0,1,2} so predictions match labels\n",
        "# ---------------------------------------------------------------------\n",
        "label_map = {orig: i for i, orig in enumerate(TARGET_CLASSES)}\n",
        "y_train = np.vectorize(label_map.get)(y_train)\n",
        "y_test  = np.vectorize(label_map.get)(y_test)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Define sizes for training / validation / test splits\n",
        "# ------------------------------------------------------------\n",
        "num_training   = 10000\n",
        "num_validation = 1000\n",
        "num_testing    = 1000\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Create subset\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Training subset\n",
        "mask = range(num_training)\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "# Validation subset\n",
        "mask = range(num_validation)\n",
        "X_val = X_test[mask]\n",
        "y_val = y_test[mask]\n",
        "\n",
        "# Test subset\n",
        "mask = range(num_validation, num_validation + num_testing)\n",
        "X_test = X_test[mask]\n",
        "y_test = y_test[mask]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Convert to float64 (optional)\n",
        "# ---------------------------------------------------------------------\n",
        "X_train = X_train.astype(np.float64)\n",
        "X_val   = X_val.astype(np.float64)\n",
        "X_test  = X_test.astype(np.float64)\n",
        "\n",
        "print(\"Shapes ->\",\n",
        "      \"X_train\", X_train.shape, \"y_train\", y_train.shape,\n",
        "      \"X_val\",   X_val.shape,   \"y_val\",   y_val.shape,\n",
        "      \"X_test\",  X_test.shape,  \"y_test\",  y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.137947Z",
          "start_time": "2022-11-28T06:46:20.972016Z"
        },
        "id": "mHATPoNJPwdQ"
      },
      "outputs": [],
      "source": [
        "def get_batch(X, y, n=1000):\n",
        "    \"Randomly select a batch of samples from the dataset.\"\n",
        "    rand_items = np.random.randint(0, X.shape[0], size=n)\n",
        "    images = X[rand_items]\n",
        "    labels = y[rand_items]\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def make_random_grid(x, y, n=4, convert_to_image=True, random_flag=True):\n",
        "    \"Create a grid of random images (or flattened vectors) from dataset.\"\n",
        "    if random_flag:\n",
        "        rand_items = np.random.randint(0, x.shape[0], size=n)\n",
        "    else:\n",
        "        rand_items = np.arange(0, x.shape[0])\n",
        "    images = x[rand_items]\n",
        "    labels = y[rand_items]\n",
        "    if convert_to_image:\n",
        "        grid = np.hstack(np.array([np.asarray((vec_2_img(i) + mean_image), dtype=np.int64) for i in images]))\n",
        "    else:\n",
        "        grid = np.hstack(np.array([np.asarray(i, dtype=np.int64) for i in images]))\n",
        "    print('\\t'.join('%9s' % classes[labels[j]] for j in range(n)))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def vec_2_img(x):\n",
        "    \"\"\" Convert a flattened CIFAR-10 image vector back to a (32, 32, 3) RGB image.\n",
        "    Removes bias term if present.\"\"\"\n",
        "    x = np.reshape(x[:-1], (32, 32, 3))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "3b5MlMr9FnIw",
        "outputId": "e5ac6578-ab68-4412-beda-e5e6c43698b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     bird\t     bird\t     bird\t     deer\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x1200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# quick sanity check preview\n",
        "X_batch, y_batch = get_batch(X_test, y_test, 100)\n",
        "plt.imshow(make_random_grid(X_batch, y_batch, n=4, convert_to_image=False))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9MRQD6v4aDz"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRuXmTuzPwdS"
      },
      "source": [
        "## Cross-entropy\n",
        "\n",
        "\n",
        "Complete the function `softmax_loss` using vectorized code. This function takes as input `scores`, labels `y` and outputs the calculated loss as a single number and the gradients with respect to scores. **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.153755Z",
          "start_time": "2022-11-28T06:46:21.140549Z"
        },
        "id": "GQ2BtZDMOaMv"
      },
      "outputs": [],
      "source": [
        "def softmax_loss(scores, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - scores: scores of shape (N, C) where scores[i, c] is the score for class c on input X[i].\n",
        "    - y: Vector of labels\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to scores\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement this function                                           #\n",
        "    ###########################################################################\n",
        "\n",
        "    \n",
        "    \n",
        "    ###########################################################################\n",
        "    #                              END OF YOUR CODE                           #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.169038Z",
          "start_time": "2022-11-28T06:46:21.155754Z"
        },
        "id": "4VLcsKfYEbor"
      },
      "outputs": [],
      "source": [
        "# some tests\n",
        "np.random.seed(42)\n",
        "\n",
        "num_instances = 5\n",
        "num_classes = 3\n",
        "\n",
        "y = np.random.randint(num_classes, size=num_instances)\n",
        "scores = np.random.randn(num_instances * num_classes).reshape(num_instances, num_classes)\n",
        "loss, dx = softmax_loss(scores, y)\n",
        "\n",
        "\n",
        "correct_grad = np.array([[ 0.0062,  0.1751, -0.1813],\n",
        "         [-0.1463,  0.0561,  0.0901],\n",
        "         [ 0.0404,  0.0771, -0.1174],\n",
        "         [ 0.0223,  0.0855, -0.1078],\n",
        "         [-0.1935,  0.1358,  0.0578]])\n",
        "correct_loss = 1.7544\n",
        "print(dx)\n",
        "\n",
        "assert np.isclose(dx.round(4), correct_grad, rtol=1e-3).all()\n",
        "assert np.isclose(loss.round(4), correct_loss, rtol=1e-3).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3b1r34XiywH"
      },
      "source": [
        "## L2 Regularization\n",
        "\n",
        "Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term to the loss to penalize larger weights.\n",
        "$$\n",
        "Loss = Loss + \\lambda  \\cdot \\frac{1}{2} \\cdot \\sum_{i=0}^k w_k^2\n",
        "$$\n",
        "\n",
        "Implement the L2 regularization part of the loss in the next cell: **(10 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.184401Z",
          "start_time": "2022-11-28T06:46:21.171489Z"
        },
        "id": "wk--K9pGi_Lb"
      },
      "outputs": [],
      "source": [
        "def l2_regulariztion_loss(W, reg=0):\n",
        "    \"\"\"\n",
        "    L2 regulariztion loss function, vectorized version.\n",
        "    - W: a layer's weights.\n",
        "    - reg: (float) regularization strength\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "    #############################################################################\n",
        "    # TODO: Compute the L2 reulariztion loss and its gradient using no\n",
        "    # explicit loops.                                                           #\n",
        "    # Store the loss in loss and the gradient in dW.                            #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                          END OF YOUR CODE                                 #\n",
        "    #############################################################################\n",
        "    return loss, dW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xerXQe6hPwdU"
      },
      "source": [
        "# Neural Network\n",
        "\n",
        "The implementation of linear regression was (hopefully) simple yet not very modular since the layer, loss and gradient were calculated as a single monolithic function. This would become impractical as we move towards bigger models. As a warmup towards `PyTorch`, we want to build networks using a more modular design so that we can implement different layer types in isolation and easily integrate them together into models with different architectures.\n",
        "\n",
        "This logic of isolation & integration is at the heart of all popular deep learning frameworks, and is based on two methods each layer holds - a forward and backward pass.\n",
        "- The forward function will receive inputs, weights and other parameters and will return both an output and a cache object storing data needed for the backward pass.\n",
        "- The backward pass will receive upstream derivatives and the cache, and will return gradients with respect to the inputs and weights.\n",
        "\n",
        "By implementing several types of layers this way, we will be able to easily combine them to build classifiers with different architectures with relative ease.\n",
        "\n",
        "The Task: In the following cells, we will implement a Neural Network to obtain better results on the CIFAR-10 dataset. We will train this network using Softmax loss, L2 regularization, and ReLU non-linearity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHH9dGgt4NP3"
      },
      "source": [
        "### Fully Connected Layer: Forward Pass.\n",
        "\n",
        "Implement the function `fc_forward`. **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.200656Z",
          "start_time": "2022-11-28T06:46:21.189432Z"
        },
        "id": "OO3vgLtGVWkv"
      },
      "outputs": [],
      "source": [
        "def fc_forward(X, W, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for an fully connected layer.\n",
        "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "    then transform it to an output vector of dimension M.\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
        "    - W: A numpy array of weights, of shape (D, M)\n",
        "    - b: A numpy array of biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    #############################################################################\n",
        "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
        "    # will need to reshape the input into rows.                                 #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    cache = (X.copy(), W.copy(), b.copy())\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.215718Z",
          "start_time": "2022-11-28T06:46:21.202326Z"
        },
        "id": "bkwG7GJ_PwdU"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "num_instances = 5\n",
        "input_shape = (11, 7, 3)\n",
        "output_shape = 4\n",
        "\n",
        "X = np.random.randn(num_instances * np.prod(input_shape)).reshape(num_instances, *input_shape)\n",
        "W = np.random.randn(np.prod(input_shape) * output_shape).reshape(np.prod(input_shape), output_shape)\n",
        "b = np.random.randn(output_shape)\n",
        "\n",
        "out, _ = fc_forward(X, W, b)\n",
        "\n",
        "correct_out = np.array([[16.77132953,  1.43667172, -15.60205534,   7.15789287],\n",
        "                        [ -8.5994206,  7.59104298,  10.92160126,  17.19394331],\n",
        "                        [ 4.77874003,  2.25606192,  -6.10944859,  14.76954561],\n",
        "                        [21.21222953, 17.82329258,   4.53431782,  -9.88327913],\n",
        "                        [18.83041801, -2.55273817,  14.08484003,  -3.99196171]])\n",
        "\n",
        "assert np.isclose(out, correct_out, rtol=1e-8).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaJgjBa2PwdU"
      },
      "source": [
        "### Fully Connected Layer: Backward Pass\n",
        "\n",
        "Implement the function `fc_backward` **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.231622Z",
          "start_time": "2022-11-28T06:46:21.217581Z"
        },
        "id": "GKGLxK7wVakI"
      },
      "outputs": [],
      "source": [
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an fully connected layer.\n",
        "    Try the link in the exercise intructions for more details.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives\n",
        "    - cache: Tuple of:\n",
        "      - X: Input data\n",
        "      - W: Weights\n",
        "      - b: Biases\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to X\n",
        "    - dw: Gradient with respect to W\n",
        "    - db: Gradient with respect to b\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = 0, 0, 0\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.247332Z",
          "start_time": "2022-11-28T06:46:21.232614Z"
        },
        "id": "K3Xcoeqxnq_F"
      },
      "outputs": [],
      "source": [
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.262753Z",
          "start_time": "2022-11-28T06:46:21.248711Z"
        },
        "id": "7J13imMzPwdU"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: fc_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: fc_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: fc_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "out, cache = fc_forward(x,w,b)\n",
        "dx, dw, db = fc_backward(dout, cache)\n",
        "\n",
        "assert np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
        "assert np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
        "assert np.isclose(db, db_num, rtol=1e-8).all() # simple test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6D5b-J5PwdV"
      },
      "source": [
        "### ReLU: Forward Pass\n",
        "\n",
        "Implement the function `relu_forward`. **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.278815Z",
          "start_time": "2022-11-28T06:46:21.264744Z"
        },
        "id": "bO5iMs3aVeTl"
      },
      "outputs": [],
      "source": [
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    #############################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                    #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    cache = x.copy()\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.294685Z",
          "start_time": "2022-11-28T06:46:21.280962Z"
        },
        "id": "WdYx8zaOPwdV"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "assert np.isclose(out, correct_out, rtol=1e-8).all() # simple test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOP-MRufPwdV"
      },
      "source": [
        "### ReLU: Backward Pass\n",
        "\n",
        "Implement the function `relu_backward`. **(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.310754Z",
          "start_time": "2022-11-28T06:46:21.296684Z"
        },
        "id": "n3YsNEphVhuo"
      },
      "outputs": [],
      "source": [
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = None, cache\n",
        "    #############################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                   #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.325592Z",
          "start_time": "2022-11-28T06:46:21.312932Z"
        },
        "id": "w_9OFZZ-PwdV"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "xx, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "\n",
        "assert np.isclose(dx, dx_num, rtol=1e-8).all()  # simple test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX0UZJdDPwdV"
      },
      "source": [
        "### Combined Layer\n",
        "Next combine the fully connected and relu forward\\backward functions togther using the functions in the following cell.\n",
        "Remember to use functions you already implemented.\n",
        "**(5 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.341148Z",
          "start_time": "2022-11-28T06:46:21.327148Z"
        },
        "id": "yhSV6tHgpZd0"
      },
      "outputs": [],
      "source": [
        "def fc_relu_forward(X, W, b):\n",
        "    \"\"\"\n",
        "    Forward pass for a fully connected layer followed by a ReLU.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Input to the fc layer\n",
        "    - W, b: Weights for the fc layer\n",
        "\n",
        "    Returns:\n",
        "    - out: Output from the ReLU\n",
        "    - cache: Object to give to the backward pass\n",
        "    \"\"\"\n",
        "    #############################################################################\n",
        "    # TODO: Implement the function.                                             #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    cache = (fc_cache, relu_cache)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Backward pass for a fully connected layer followed by a ReLU\n",
        "    Inputs:\n",
        "    - dout: upstream derivatives\n",
        "    - cache: parameters calculated during the forward pass\n",
        "\n",
        "    Returns:\n",
        "    - dX: derivative w.r.t X\n",
        "    - dW: derivative w.r.t W\n",
        "    - db: derivative w.r.t b\n",
        "    \"\"\"\n",
        "    fc_cache, relu_cache = cache\n",
        "    #############################################################################\n",
        "    # TODO: Implement the function.                                             #\n",
        "    #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #############################################################################\n",
        "    #                             END OF YOUR CODE                              #\n",
        "    #############################################################################\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RNFsQVGrFUE"
      },
      "source": [
        "You can check your results in the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.357151Z",
          "start_time": "2022-11-28T06:46:21.343147Z"
        },
        "id": "wfsIy8dEqx7r"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: fc_relu_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: fc_relu_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: fc_relu_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "out, cache = fc_relu_forward(x,w,b)\n",
        "dx, dw, db = fc_relu_backward(dout, cache)\n",
        "\n",
        "assert np.isclose(dw, dw_num, rtol=1e-8).all() # simple test\n",
        "assert np.isclose(dx, dx_num, rtol=1e-8).all() # simple test\n",
        "assert np.isclose(db, db_num, rtol=1e-8).all() # simple test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHYeDvNcPwdV"
      },
      "source": [
        "# Building the Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7-k0EkePwdV"
      },
      "source": [
        "Complete the class `ThreeLayerNet`. **(25 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.388140Z",
          "start_time": "2022-11-28T06:46:21.358811Z"
        },
        "id": "883fce5uWDVl"
      },
      "outputs": [],
      "source": [
        "class ThreeLayerNet(object):\n",
        "    \"\"\"\n",
        "    A three-layer fully-connected neural network. This network has an input dimension of\n",
        "    N, a hidden layer dimension of H, and performs classification over C classes.\n",
        "    In our case, we use the same hidden dimension across all hidden layers.\n",
        "    We train the network with a softmax loss function and L2 regularization on the\n",
        "    weight matrices. In other words, the network has the following architecture:\n",
        "\n",
        "    input - fc layer - ReLU - fc layer - ReLu - fc layer - softmax\n",
        "\n",
        "    The outputs of the third fully-connected layer are the scores for each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, std=1e-2):\n",
        "        \"\"\"\n",
        "        Initialize the model. Weights are initialized to small random values and\n",
        "        biases are initialized to zero. Weights and biases are stored in the\n",
        "        variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, H)\n",
        "        b2: Second layer biases; has shape (H,)\n",
        "        W3: Second layer weights; has shape (H, C)\n",
        "        b3: Second layer biases; has shape (C,)\n",
        "\n",
        "        Inputs:\n",
        "        - input_size: The dimension D of the input data.\n",
        "        - hidden_size: The number of neurons H in each of the hidden layers.\n",
        "        - output_size: The number of classes C.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = std * np.random.randn(hidden_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "    def step(self, X, y=None, reg=0.0):\n",
        "        \"\"\"\n",
        "        Compute the loss and gradients for a three layer fully connected neural\n",
        "        network.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. This parameter is optional; if it\n",
        "          is not passed then we only return scores, and if it is passed then we\n",
        "          instead return the loss and gradients.\n",
        "        - reg: Regularization coefficient.\n",
        "\n",
        "        Returns:\n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "          samples.\n",
        "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "          with respect to the loss function; has the same keys as self.params.\n",
        "        \"\"\"\n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        W3, b3 = self.params['W3'], self.params['b3']\n",
        "\n",
        "        # Compute the forward pass\n",
        "        scores = None\n",
        "        #############################################################################\n",
        "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
        "        # Store the result in the scores variable, which should be an array of      #\n",
        "        # shape (N, C).                                                             #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                              END OF YOUR CODE                             #\n",
        "        #############################################################################\n",
        "\n",
        "        # If the targets are not given then jump out, we're done\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = None\n",
        "        ###############################################################################\n",
        "        # After you finished the forward pass, compute the loss. This should include  #\n",
        "        # both the data loss and L2 regularization for W1, W2, W3. Store the result   #\n",
        "        # in the variable loss, which should be a scalar. Use the softmax_loss        #\n",
        "        # and l2_regulariztion_loss functions you implemented.                        #\n",
        "        ###############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                              END OF YOUR CODE                             #\n",
        "        #############################################################################\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        grads = {}\n",
        "        #############################################################################\n",
        "        # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
        "        # and biases. Store the results in the grads dictionary. For example,       #\n",
        "        # grads['W1'] = dW1 + dW1_reg, it stores the gradient on W1, including      #\n",
        "        # regularization. It should be a matrix of the same size.                   #\n",
        "        #############################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #############################################################################\n",
        "        #                              END OF YOUR CODE                             #\n",
        "        #############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "    def train(self, X, y, X_val, y_val,\n",
        "              learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
        "              batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this neural network using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving training data.\n",
        "        - y: A numpy array f shape (N,) giving training label.\n",
        "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
        "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
        "        - learning_rate: Scalar giving learning rate for optimization.\n",
        "        - reg: Scalar giving regularization strength.\n",
        "        - num_iters: Number of steps to take when optimizing.\n",
        "        - batch_size: Number of training examples to use per step.\n",
        "        - verbose: boolean; if true print progress during optimization.\n",
        "        \"\"\"\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "        # Use SGD to optimize the parameters in self.model\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "            #########################################################################\n",
        "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
        "            # them in X_batch and y_batch respectively.                             #\n",
        "            #########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #########################################################################\n",
        "            #                             END OF YOUR CODE                          #\n",
        "            #########################################################################\n",
        "\n",
        "            # Compute loss and gradients using the current minibatch\n",
        "            loss, grads = self.step(X_batch, y=y_batch, reg=reg)\n",
        "            loss_history.append(loss)\n",
        "            #########################################################################\n",
        "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "            # parameters of the network (stored in the dictionary self.params)      #\n",
        "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "            # stored in the grads dictionary defined above.                         #\n",
        "            #########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #########################################################################\n",
        "            #                             END OF YOUR CODE                          #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and (it+1) % 100 == 0:\n",
        "                print ('iteration %d / %d: loss %f' % (it+1, num_iters, loss))\n",
        "\n",
        "            # Every epoch, check train and val accuracy.\n",
        "            if it % iterations_per_epoch == 0:\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "\n",
        "        return {\n",
        "          'loss_history': loss_history,\n",
        "          'train_acc_history': train_acc_history,\n",
        "          'val_acc_history': val_acc_history,\n",
        "        }\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this three-layer network to predict labels for\n",
        "        data points. For each data point we predict scores for each of the C\n",
        "        classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "        Inputs:\n",
        "        - X: data points to classify.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: predicted labels\n",
        "        \"\"\"\n",
        "        y_pred = None\n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        W3, b3 = self.params['W3'], self.params['b3']\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this function                                           #\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                              END OF YOUR CODE                           #\n",
        "        ###########################################################################\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:46:21.403237Z",
          "start_time": "2022-11-28T06:46:21.390103Z"
        },
        "id": "i6KXQ1KvPwdV"
      },
      "outputs": [],
      "source": [
        "input_size = 32 * 32 * 3\n",
        "hidden_size = 128\n",
        "num_classes = 4\n",
        "model = ThreeLayerNet(input_size, hidden_size, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:47:07.708024Z",
          "start_time": "2022-11-28T06:46:21.405440Z"
        },
        "id": "XbUwlaa9PwdV"
      },
      "outputs": [],
      "source": [
        "stats = model.train(X_train, y_train, X_val, y_val,\n",
        "            num_iters=1500, batch_size=200,\n",
        "            learning_rate=1e-3, reg=0, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:47:07.754799Z",
          "start_time": "2022-11-28T06:47:07.711049Z"
        },
        "id": "-1hDhsb1PwdV"
      },
      "outputs": [],
      "source": [
        "val_acc = (model.predict(X_val) == y_val).mean()\n",
        "print ('Validation accuracy: ', val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:47:08.208742Z",
          "start_time": "2022-11-28T06:47:07.756434Z"
        },
        "id": "VdhExUOrKcc6"
      },
      "outputs": [],
      "source": [
        "train_acc = (model.predict(X_train) == y_train).mean()\n",
        "print ('Training accuracy: ', train_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T06:47:08.619195Z",
          "start_time": "2022-11-28T06:47:08.210823Z"
        },
        "id": "F-9rpmQAPwdW"
      },
      "outputs": [],
      "source": [
        "# Plot the loss function and train / validation accuracies\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(stats['loss_history'])\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(stats['train_acc_history'], label='train')\n",
        "plt.plot(stats['val_acc_history'], label='val')\n",
        "plt.title('Classification accuracy history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Clasification accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okkhr5xjPwdW"
      },
      "source": [
        "## Hyperparameter Optimization\n",
        "Use the validation set to tune hyperparameters by training different models (using the training dataset) and evaluating the performance using the validation dataset. Save the results in a dictionary mapping tuples of the form `(learning_rate, hidden_size, regularization)` to tuples of the form `(training_accuracy, validation_accuracy)`. You should evaluate the best model on the testing dataset and print out the training, validation and testing accuracies for each of the models and provide a clear visualization. Highlight the best model w.r.t the testing accuracy. **(10 Points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-28T07:02:08.693145Z",
          "start_time": "2022-11-28T06:47:08.623196Z"
        },
        "id": "WU33Q_kwPwdW"
      },
      "outputs": [],
      "source": [
        "# This might take some time, try to expirement with small number of testing parameters before continuing\n",
        "# You are encouraged to experiment with additional values\n",
        "learning_rates = [1e-4, 1e-3]\n",
        "hidden_sizes = [32, 64, 128, 256]\n",
        "regularizations = [0, 0.001, 0.1, 0.25]\n",
        "\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_net = None\n",
        "################################################################################\n",
        "#                            START OF YOUR CODE                                #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "\n",
        "# Print out results.\n",
        "for lr, hidden_size, reg  in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, hidden_size, reg)]\n",
        "    print ('lr %e hidden_size %f reg %f train accuracy: %f val accuracy: %f' % (\n",
        "                lr, hidden_size, reg, train_accuracy, val_accuracy))\n",
        "\n",
        "print ('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
        "\n",
        "test_accuracy = (best_net.predict(X_test) == y_test).mean()\n",
        "print ('Neural Network on raw pixels final test set accuracy: %f' % test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziBGoynPsENS"
      },
      "source": [
        "## Best setup 1\n",
        "\n",
        "\n",
        "Run your best setup with 3 different seeds. Report mean±std for F1. Are results stable? If not, likely causes variance? (provid our answer + code)\n",
        "\n",
        "**(3 Points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnGLZwE1r5DP"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bmfa5IDtrwmg"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MELOjxeHrI7g"
      },
      "source": [
        "## Best setup 2\n",
        "\n",
        "Run a learning curve for this homework by training on 10% / 30% / 50% / 100% of the training set (use 1 seed).\n",
        "\n",
        "- Where does performance start to saturate?\n",
        "\n",
        "- Does adding data help precision or recall more?\n",
        "\n",
        "- Based on the curve’s shape, argue whether the current model is more bias-limited or variance-limited, and name one actionable change you’d try next.\n",
        "- (provid our answer + code)\n",
        "\n",
        "**(9 Points)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV2SeaEbr9VL"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3B_ThhJr9wR"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkyH1MtdPwdW"
      },
      "source": [
        "# Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlFFcw7aEbo-"
      },
      "source": [
        "##**Question 1:**\n",
        "Compare HW1 and HW2 along training time, performance, and tuning difficulty. Back claims with evidence.\n",
        "\n",
        "- Training time\n",
        "\n",
        "  Which modle took longer to train? Explain why one took longer (e.g., dataset size, model capacity, optimizer, I/O, augmentation).\n",
        "\n",
        "- Performance\n",
        "\n",
        "  Compare the main metric used in each HW (e.g., Accuracy/Macro-F1) on the validation set. which preduced better results and why?\n",
        "\n",
        "- Tuning difficulty\n",
        "\n",
        "  State which HW was harder to tune and why (search space size, instability, variance).\n",
        "\n",
        "- One-line takeaway\n",
        "\n",
        "  A single, precise sentence: “HW2 improved X by Y but cost Z (time/complexity), mainly due to …”\n",
        "\n",
        "**(7 Points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbJWJAsdwhzH"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0RygMPoEbo-"
      },
      "source": [
        "##**Question 2:**\n",
        "\n",
        "What can you say about the diffrence (or lack of thereof) between the validation and training accuracy?\n",
        "**(3 Points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUHA6CgKye1F"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "What can you say about the connection between the loss and the accuracy? **(3 Points)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnlvFjWWwr5j"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh8-pdw3-3u7"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Oa-bjEWl9fW6",
        "8MeVWzF19mVA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
